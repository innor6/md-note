# 线性代数

### 矩阵乘法

向量的内积：
$$
x^T y \in \R 
= 
\begin{bmatrix}
x_1 & x_2 & \dots & x_n
\end{bmatrix}

\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix} 

= \sum_{i=1}^nx_iy_i
$$

相似性：如果值为正数，则说明两个向量在方向上相似；如果为0，则正交。



向量的外积：
$$
xy^T \in \R^{m \times n}
=
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_m
\end{bmatrix} 

\begin{bmatrix}
y_1 & y_2 & \dots & y_n
\end{bmatrix}

= 
(m \times n) \  Matrix
$$
结果是秩一矩阵。k个m*n的秩一矩阵相加，得到的矩阵的rank = min(m, n, k)，如果是线性相关的则rank会更小。



矩阵和矩阵的乘积：

1. 结果的矩阵中的每个元素值：可以看成是行向量和列向量的内积；
2. 整个矩阵：可以看成是多个列向量和行向量的外积得到的矩阵相加。



### 矩阵是变换

Ax = b：矩阵A将空间中的向量x变换到向量b

- 如果A是满秩的，空间中任意的向量都可以唯一的映射到空间中的另一个向量（双射），$A^{-1}$ 则是从向量b到向量x的逆映射；
- 如果A是不满秩的，则存在单射，空间中存在一个低维的子空间（行空间），该子空间上的向量都被映射到空间中的另一个低维子空间（列空间）。（想象从三维空间中一个平面上的向量都被映射到另一个平面上）
  - 空间中所有的向量经过A变换只能映射到列空间。如果向量x不在行空间上，则它经过A变换后的向量等价于：先把x投影到行空间后再进行A变换得到的向量；
  - 反过来，列空间的向量只能逆映射到行空间，列空间之外的向量无法被逆映射。

行空间：矩阵A的行向量通过线性组合构成的空间

列空间：矩阵A的列向量通过线性组合构成的空间

零空间：满足Ax = 0 的所有 x



可逆矩阵：满秩且是方阵



### 投影

向量y在向量a上的投影
$$
Proj(y; a) = \frac{aa^T}{a^Ta} y
$$
其中 $a^Ta$ 是向量 a 的模，$aa^T$ 是秩一矩阵，这个分式称为投影矩阵。



向量y在平面上的投影
$$
Proj(y;X) = [X(X^TX)^{-1}X^T]y
$$
所投影到的平面是矩阵X的列空间，即用一组平面的基向量来构造矩阵X，这个投影矩阵的形式其实类似上面的分式。



考虑线性回归问题
$$
X\theta = y
$$
对于给定的训练集X，我们知道任意的参数θ经过X变换后只会映射到X的列空间，而列空间中最接近y的向量，是y在列空间上的投影。因此我们选择这样的θ：使得Xθ是y在列空间上的投影，此时预测值与实际值的误差最小。
$$
X\theta = X[(X^TX)^{-1}X^Ty]
$$

从而得到正则化方法的一步求出 $θ = (X^TX)^{-1}X^Ty$

（思考：投影的本质也是最近距离，与最小二乘法不谋而合）



### 矩阵分解

矩阵变换都可以分解为三步：旋转1，缩放，旋转2

这三个矩阵分别为：正交矩阵，对角矩阵，正交矩阵

- Eigen分解：旋转，以复数缩放，逆旋转回去
- SVD：旋转1，以实数缩放，旋转2

性质：

1. 对称矩阵的Eigen分解和SVD结果是一样的。
2. 特征向量：Eigen分解时，先旋转，然后沿着相互正交的轴拉伸，再旋转回去，则这些在缩放时正交的向量，经过旋转后还是正交的，这些在变换过程中始终保持正交的向量称为特征向量，缩放的大小称为特征值（谱）。
   1. 特征值之和 = trace = 对角线之和
   2. 特征值之积 = 行列式（变换前后的体积的比值）



# 矩阵求导

$$
f(x) = \frac{1}{2}x^TAx + b^Tx \\
\nabla f(x) = Ax + b \\
\nabla^2 f(x) = A \\
\nabla (a^Tx) = a
$$

Hessien矩阵
$$
H_{jk} = \frac{\part^2 g(h(x))}{\part \theta_i \part \theta_j} 
= \frac{\part^2 g(h(x))}{\part (h(x))^2} \frac{\part h(x)}{\part x_i} \frac{\part h(x)}{\part x_j}
= g''(h(x)) \frac{\part h(x)}{\part x_i} \frac{\part h(x)}{\part x_j}
$$




# 半正定 PSD

$A = A^T$ 且 $x^TAx \ge 0$

1. 如果$A = zz^T$，则A是PSD。
2. 如果A是PSD，则BAB^T也是PSD，B是任意符合维度的矩阵。



# 特征向量

定义：对于矩阵方A，所有满足 Ax = λx 的 (x, λ) 称为A的特征值、特征向量。（特征向量x经过A变换后方向不变，长度拉伸为λ）

对角化：$A = TΛT^{-1}$，其中Λ是特征值构成的对角矩阵，T是可逆矩阵，是一组特征向量 T = [t1 t2 ...]

正交化：如果A是对称矩阵，$A = A^T$，则A可以被一个实数正交矩阵对角化：$A = UΛU^{-1}$，其中U是一组正交的特征向量。

