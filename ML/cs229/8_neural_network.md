1. neuron = linear + activation（线性模型 + 激活函数）
2. model = architecture + parameter



激活函数：

1. sigmoid：$g(z)=\frac{1}{1+e^{-z}}，g' = g(1 - g)$
2. ReLU：$g(z)=max(z,0)，g'=1$
3. tanh：$g(z)=\frac{e^z - e^{-z}}{e^z + e^{-z}}，g'=1-tanh^2$

说明：

1. 当 z 过大或过小时，sigmoid 和 tanh 会出现梯度消失的问题，导致收敛速度变慢，而 ReLU 则不会
2. sigmoid 和 tanh 选择哪个，取决于你希望的输出的值域是什么



输入层、隐藏层、输出层



TODO：求解反向传播，sigmod求导为什么是矩阵的元素乘法，顺序、维度怎么判断？



初始化方法：

1. 对输入标准化：输入减去均值，除以方差。好处是使得数据分布接近正圆形，随机梯度下降时总是向中心下降，减少迭代次数。（注意这里的均值和方差是通过训练集得到的，而在预测时还得用这里得到的数值进行标准化）
2. 梯度消失、梯度爆炸：
   1. w初始化为根号1/n，如果使用ReLU则用根号2/n
   2. w初始化为$\sqrt{\frac{2}{n^L + n^{L-1}}}$，L表示第L层的神经元

3. 动量：梯度下降时，考虑前一次下降的方向，从而使得每次下降的方向更有趋向性，避免Z字形的下降。方向 $v = \beta v + (1-\beta)\frac{\part L}{\part w}$
   

