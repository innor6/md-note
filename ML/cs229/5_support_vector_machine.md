# Support Vector Machines

几何直觉：用一个超平面（决策边界）来对输入进行分类，显然离超平面越远的输入，它被分离正确的置信度更高。因此要尽量最大化超平面的边距。

##### 记号

- 二分类问题：$y \in \{-1,1\}$
- 预测函数： $h_{w,b}(x) = g(w^T x + b)$ （其实就是将之前的 θ 分成了 w 和 b）
- 其中 g(z)：根据z的正负输出+1或-1

##### 函数边距

$$
\hat{\gamma} = y * (w^T x + b)
$$

1. 当 y=1，则后半部分应该为尽量大的正数
2. 当 y=-1，则后半部分应该为尽量小的负数
3. γ表示输入离决策边界的距离，即置信度

##### 几何边距

$$
\gamma = \frac{y * (w^T x + b)}{||w||}
$$

1. 通过几何矢量计算得出
2. 其实就是函数边距除以|w|，因此当 |w| = 1 时，两者相等。
3. 特性：由于除以了|w|，随意的缩放参数 w和 b 都不会改变几何边距 γ 的大小。

##### 训练集S的边距

S中所有样本中，边距最小的那个边距。

##### 支持向量

指最靠近分离边界的那几个样本。

实际上在预测的时候，有：
$$
w^T x + b = \sum_{i=1}^n \alpha_i y^{(i)} \langle x^{(i)}, x \rangle + b
$$
通过拉格朗日对偶的优化方法证明只有支持向量对应的α非0，因此<u>只需要计算输入x与那几个支持向量的内积</u>并作计算即可。



### The optimal margin classifier

最优化问题：最大化训练集S的最小几何边距（非凸问题）

利用 ω 和 b 可缩放而几何边距不变的特性，保持训练集S的最小函数边距为1，转化为的凸的优化问题：
$$
\begin{align}
\min_{w,b} &\quad \frac{1}{2}||w||^2 \\
s.t. &\quad y^{(i)}(w^T x^{(i)} + b) \ge 1
\end{align}
$$
扩展：利用拉格朗日对偶性可以更高效的解决这个问题。



（可能在 optimal margin classifiers）

直觉

1. 参数ω是所有训练样本x的线性组合。

$$
w = \sum_{i=1}^n \alpha_i y^{(i)}x^{(i)}
$$

（考虑梯度下降过程，参数 θ 从零向量开始，每次迭代都是加减一个x的缩放：
$$
\theta := \theta - \alpha(h_\theta(x^{(i)}) - y^{(i)})x^{(i)}
$$

2. ω是决策边界的法向量。



### Kernel

##### 映射 φ

将输入向量x映射为高维向量φ(x)

##### 核函数 K

K(x,z) = φ(x)^T φ(z)

有时计算 K 的代价可能比直接计算 φ 要小得多。利用这一特性我们可以高效的计算高维向量的内积。

例如：
$$
K(x,z) = (x^T z)^2 = \sum_{i,j=1}^n (x_ix_j)(z_iz_j) = \phi(x)^T\phi(z)
$$
这里 φ(x) 有 n^2 维，计算代价为 O(n^2)，而 K 的计算代价只有 O(n)。

同理还有：（其中第二个是表示φ(x)的分量为n个xi中选d个相乘）
$$
K(x,z) = (x^T z + c)^2 \\
K(x,z) = (x^T z + c)^d
$$


直觉上，核函数 K(x,z) 描述了 x,z 的相似性（如内积），x,z 越相似，K往往越大，于是定义：
$$
K(x,z) = \exp(-\frac{||x-z||^2}{2\sigma^2})
$$

1. 高斯核函数，对应映射到无限维的φ
2. x和z相似时，接近1
3. x和z远离时，接近0

Mercer 定理：当且仅当任意n个输入 x 的 kernel 矩阵（K_ij = K(xi,xj)）是对称半正定时，K是一个有效的核函数。



### Kernel Trick

此方法可以与许多算法结合，从而使他们可以在高维空间高效运行。

1. 将算法写成样本之间内积 <x,z> 的形式
2. 将样本x映射到高维φ(x)（可以是无限维）
3. 找到一个高效方法去计算核函数 K(x,z) = φ(x)^T φ(z)
4. 用 K(x,z) 来替换算法中的 <x,z>（由于已经求出了K的表达式，从而不需要去做高维映射φ(x)了）



SVM = optimal margin classifier + Kernel Trick

将x映射到d维，而计算的时间复杂度只有根号d。

好处：低维度下的非线性分类，可以通过将其映射到高维度，在高维空间中找一个线性的决策边界。

##### 为什么SVM在高维下不会过拟合？

可能是最小化目标|w|^2在一定程度上阻止了过拟合。



L1 norm soft margin SVM：允许一部分样本离分离边界更近一些，甚至越过边界
