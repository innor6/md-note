# Generalized Linear Models（GML）

### 指数族

指随机变量 y 满足如下的形式的PDF：
$$
p(y; \eta) = b(y) \exp(\eta^TT(y) - \alpha(\eta))
$$
其中，

- 注意 y 是数据（其他的地方一般用x表示）
- η 是自然参数（相较于学习中的参数θ、常用pdf里的常用参数）
- T(y) 是充分统计（sufficient statistic），通常 T(y) 就是 y
- α(η) 是对数划分函数（log partition function），可以将这一项分离出来放在分母，其实就是用于将概率归一化的一个常数



确定了T(y)、α(η)、b(y)，可以得到一系列的分布函数，通过改变自然参数 η 我们可以得到了一个指数族。

常用的分布的PDF都可以转化为上面的形式。



它们有很好的性质：

1. 优化问题是凹函数，即关于参数 η 的MLE（最大似然估计）是凹函数；NLL（负数的log似然估计，如cost function）是凸函数
2. a(η)的一阶导是均值：$E(y; \eta) = \frac{\partial}{\partial \eta} a(\eta)$
3. a(η)的二阶导是方差：$Var(y; \eta) = \frac{\partial^2}{\partial \eta^2} a(\eta)$



使用：

实数——高斯分布

二分类——伯努利分布

非负整数——泊松分布

正整数——gamma、指数分布

概率分布——beta、dirichlet



### GLM

为了将指数族扩展到GLM，假设：

1. （对于给定的x，实际的y的分布与指数族独立同分布）y | x; θ ~ 指数族(η) 
2. （自然参数η与x线性相关）$\eta = \theta^T x$
3. （检验学习成果时，希望预测的T(y)等于均值，通常 T(y) 就是 y）y = h(x) = E(y|η) 

解释：

通过训练（如极大似然估计）得到 θ，对于每个输入x，都可以得到 η = θx；对于给定的输入η（也就是x），我们希望预测得到的y是真实的y的均值。

使用说明：

1. 一旦选定了分布，则期望 E(y|η) 可以用对应分布的canonical参数来表示（如伯努利的Φ、高斯的μ）；
2. 然后根据指数族，我们可以得到canonical参数——自然参数 η 的关系；
3. 而 η = θx，从而我们就建立起了 θx 到 E(y|η) 的桥梁，根据 T(y) = h(x) = E(y|η) ，我们就可以知道不同分布下应该如何选择合适的 h。（如逻辑回归中用的sigmoid函数其实就是伯努利家族的均值）



无论使用了哪种分布，学习的更新规则都是：
$$
\theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}
$$
只需要选择不同的 h 即可开始学习。