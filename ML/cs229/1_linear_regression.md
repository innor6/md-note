监督学习：

对于已经标定了输入和输出的训练集 x -> y，我们希望找出一个近似函数 h，使得 h(x) 的输出接近实际值 y。



回归 vs 分类：

- 回归：y是连续的函数
- 分类：y是离散的值



loss function：单个输入上的误差

cost function：一批输入上的误差



# 线性回归

通过线性函数 h 来近似 y：
$$
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2
$$
假设 x0 = 1，可记为：
$$
h(x) = \sum_{i=0}^{d}\theta_ix_i = \theta^Tx
$$
其中 θ 和 x 是 d 维的向量，分别表示参数（权重）和输入。



### 评价

定义一种 cost function J(θ) ：
$$
J(\theta) = \frac{1}{2}\sum_{i=1}^{n}(h_\theta(x^{(i)}) - y^{(i)})^2
$$
说明：最小二乘法。n个输入样本，每个预测值和实际值的差的平方之和。这个函数的值越小，说明 h 和 y 的误差越小。前面乘以1/2是为了方便求导化简。



##### 为什么是最小二乘？

一种概率角度的解释：设 $y^{(i)} = \theta^Tx^{(i)} + \epsilon^{(i)}$，假设随机噪声服从 $\epsilon \sim \mathcal{N} (0, \sigma^2)$，可得到 ε 的概率密度函数，从而得到似然函数，根据最大似然估计 MLE，求似然函数的最大值，取log后，发现正好就是求这个 J(θ) 的最小值。



##### 极大似然估计

对于固定的一组样本(x1, x2, ..., xn)，挑选参数θ，使得 $L(\theta) = \prod_{i=1}^n p(x_i; \theta)$ 最大，通常会取对数将其化为求和的形式，然后通过导数=0求最值。



##### arg min 符号

使得式子取到最小值时，变量z应取的值。

例如下面式子在 z=5 时取到最小值：
$$
min_z (z-5)^2 = 0 \\
arg\ min_z (z-5)^2 = 5
$$




### 学习

目标：最小化 J(θ)

J(θ) 中训练集 (x, y) 是确定的，因此需要确定参数 θ，以最小化 J(θ)。

梯度下降，随意确定一个向量 θ 的初始值，然后 θ 的每一维用下面的公式迭代，直至收敛：
$$
\begin{align}
\theta_j &:= \theta_j - \alpha\frac{\partial}{\partial \theta_j} J(\theta) \\

&:= \theta_j - \alpha\sum_{i=1}^{n}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
\end{align}
$$
说明：

1. α 是学习率，用于控制梯度下降的步长。
2. 每次下降的幅度都与预测值 h 和 实际值 y 的误差成正比。
3. 为什么梯度下降可以得到全局最优：因为对于用最小二乘法表示的cost function J(θ)，它的函数图像是碗形的。
4. 为什么 θi 在迭代更新时，要减去它的偏导而不是加上：因为当偏导为正时，说明在 θi 方向上继续前进增加会导致 J(θ) 增大，而我们的目标是最小化 J(θ) ，因此应该往反方向后退。



### 几何解释

画一条直线（h），计算每个输入x在直线上的值 h(x) 和实际值 y 的距离，目标是让距离的平方和最小化，学习过程就是让直线不断的靠近真实的趋势。



### 缺点

批量梯度下降在每次迭代，都要扫描整个训练集以计算所有 h(x) - y 的和，当数据量很大时，迭代一次会花很久。



### 随机梯度下降

每次迭代时，随机选择一个或一小部分的输入样本来更新 θ，虽然下降方向不是法线导致迭代次数增加，但能大大减少每次迭代的时间。（注意由于样本量少了，因此学习率 α 要相应的增大）

同样的还有“小批量梯度下降”，每次只随机选择一小部分的输入样本用来更新。



### 局部加权回归（LWR）

locally weighted regression

定义 cost function J(θ) ：
$$
\sum_i \omega^{(i)}(y^{(i)} - \theta^T x^{(i)})^2
$$
其中 ω 表示权重，一种选择是：
$$
\omega^{(i)} = \exp \left (  -\frac{(x^{(i)} - x)^2}{2\tau^2} \right ) 
$$

1.  图像是高斯钟形曲线，以要预测的输入 x 为中心，它表示预测输入 x 附近的训练集具有更大的参考价值，因而拥有更大的权重
2.  当 x 离 xi 接近时，ω ≈ 1
3.  当 x 离 xi 很远时，ω ≈ 0
4.  τ 控制了钟形曲线的宽度，可能导致过拟合、欠拟合，也被称为带宽参数



适合：特征数比较少（2~3），数据量很大（几千个）



局部加权回归是“非参数算法”

- 参数算法：含有有限个、固定的的参数 θi，一旦得到了 θ，可以丢弃原始训练数据，只使用 θ 就可以进行预测。如线性回归。

- 非参数算法：为了使用局部加权回归，必须保存整个训练集，h 的大小随着训练集的增长线性增长。











