非线性模型

由于熵的函数曲线，是一个凸的曲线，因此每次（有效的的？）划分之后，必然会导致总熵的下降？



缺点：

1. 每次决策只根据单个特征判断，当多个特征之间存在加性相互作用的时候（一个判断表达式里需要同时考虑多个特征来分类），分类效果大大降低，且被迫使用更深跟复杂的节点链
2. high variance，容易过拟合
3. 预测的准确性差



优点

1. 易解释
2. 快



##### bagging-bootstraping

1. 训练集S抽样出一个子集Z，用Z进行训练（bias会略微增大）；
2. 重复，每次抽样得到的Z都不同
3. 最后取这几个训练模型的均值作为结果（降低variance）

随机森林：在每次结点分裂的时候，只允许考虑随机一部分特征（使得多棵决策树不相关）



##### boosting

先训练一次

对决策中错误分类的样本，加大权重，使得下次训练的时候，受到penalize，从而选择其他的特征来决策