术语：

epoch：把所有数据看过一遍

one-hot vector：只有一个1，其他都为0的向量，常用于表示类别

data augmentation：将数据缩放、翻转、旋转得到新的训练数据，也加入到训练集。

soft-max：一种归一化的方法，对所有输入x取指数e^x，然后计算 $z_1 = \frac{\exp(x_1)}{\sum \exp(x_i)}$ ，常用于norm分类问题的输出（个人理解：有点像max()，只是这里是通过指数放大，最大的那个值会指数变大，拉开与其他较小的值的距离；置信度？）



编程：

如何正则化？

如何 batch normalization？





##### 损失函数

回归问题：最小二乘法

分类问题：通常用cross-entropy

都可以通过MLE得到。



##### 梯度下降

$$
\theta_j^{k+1} := \theta_j^k - η\frac{\partial L(\theta)}{\partial \theta_j} |_{\theta=\theta^k}
$$

1. 只能找到局部最优解（如果）
2. 随机梯度下降：每次选择一批数据用于梯度下降
3. 梯度消失



##### 训练过程

1. 定义模型
2. 定义损失函数
3. 最优化



##### 验证

train，validation，test-set

n-fold



##### 如何改进模型

- 训练loss偏大
  - bias：使用更复杂的模型。（训练样本再增多依然没有提升）
  - 最优化算法存在问题（使用了更加精细的同类型模型，loss反而增大）

- 训练loss偏小
  - 测试loss大
    - 过拟合：给模型增加限制
      1. 减少参数
      2. 减少特征
      3. 提前终止
      4. 正则化
      5. dropout
    - mismatch：训练样本和测试样本的分布不同
  - 测试loss也小：perfect！

将 train-set 分为 train 和 validate，或用 k-fold 交叉验证来。



##### critical point：局部最小值 vs 鞍点

（梯度 g = 0 的点）

如何区分？

在 θ' 处泰勒展开，附近的点的值为：
$$
L(\theta) \approx L(\theta') +  (\theta - \theta')g(\theta') + \frac{1}{2}(\theta - \theta')^T H(\theta - \theta')
$$
当 g(θ') = 0 时，第二项省略，

1. 如果 H 是正定矩阵（特征值都是正数），则周围的点 L(θ) 总数大于 L(θ')，说明 θ' 是局部最小值；
2. 反之如果特征值都是负数，则 θ' 是局部最大值；
3. 如果特征值有正有负，是鞍点。

gradient norm, minima ratio

（正的特征值的比例越大，越有可能是局部最小值）





利用特征向量 u 在鞍点下降：
$$
u^THu = u^T(\lambda u) = \lambda||u||^2
$$
根据泰勒展开，如果 λ < 0 ，说明 u = (θ - θ') 方向的 L(θ) 比鞍点低。我们只要向着负值的特征向量的方向走，就能继续下降！

（但是Hessain矩阵计算代价太大）



##### batch & momentum

有GPU并行运算时，大小batch的运算时间差不多；

小的batch在每次梯度下降的方向比较随机，可能使得梯度下降更加有效、泛化效果也较好。



带有动量的梯度下降：
$$
m := \lambda m - η g \\
θ := θ + m
$$


##### learning rate

太大会震荡。

动态调整：

RMSProp，除以一个σ：
$$
\sigma = \sqrt{α*(\sigma)^2 + (1-α)*g^2} \\
θ := θ - \frac{η}{σ}g
$$
说明：梯度增大，η增大，学习率变小。

Adam算法：RSMProp + Momentum



learning rate decay：让学习率η随时间逐渐减小

warm up：学习率先增大，后减小



##### batch normalization

（可以使得error surface变得不崎岖）

z = b+wx，在经过激活函数之前，对z也要做标准化：
$$
z = \frac{z-\mu}{\sigma}
$$
训练过程中，μ和σ可以是有记忆的，最后训练完毕时的μ和σ被用于预测时使用。





# CNN

一张图片转化为n×n×3的向量作为NN的输入。

预处理的时候可能要做scale、padding。



CNN为图像设计而做了简化：

1. receptive field：特征通常只出现在图像的一部分，因此让每个neuron只接收一个区域的输入。
   1. 用一群neuron检查一个区域 kernel-size=3*3；
   2. 每个neuron负责检测一种特征；
   3. 区域之间会有重叠，通常只是平移 stride=1~2个像素。
2. 共享参数：同一个特征会出现在图像的不同区域，因此让不同区域中检测相同特征的neuron共享参数
   1. 检测区域中某个特征的参数称为 filter
   2. 不同区域经过同一个 filter 的输出构成一个 channel
   3. 检测多少特征，输出中就有多少给 channel
3. pooling：（可选）一张图片缩放后代表的对象不变，因此可以进行下采样，减少运算量。
   1. 如将卷积后的输出以2*2的方块分割，每个方块中取一个最大值/均值，作为pooling后的输出。
   2. 通常做几次卷积后，就做一次pooling



不用担心field太小不能覆盖特征：

如果将第一层检测3*3区域的A特征的neuron的输出放在一起作为一个通道，则当用filter检查第二层的某个3\*3区域的时候，其实相当于检查了第一层的5\*5区域。



CNN不能处理缩放、旋转后的图像：

1. data augmentation，将图像缩放、旋转后交给CNN学习
2. spatial transformer layer



# Self-Attention

输入一个向量序列，输出一个向量序列。

每个输出都是**考虑序列中全部的输入向量**后产生的。

可以用于输入层，也可以用于隐藏层。

文本、音频、图像、图等能表示成向量序列的输入都可用self-attention



##### 流程

输入是一个向量序列，输出是与序列数目相同数量的结果：

1. 考虑一个样本包含4个输入向量：$I = [a1, a2, a3, a4]$；
2. 三个矩阵 Wq, Wk, Wv 去乘输入向量，如乘 a1 得到 q1,k1,v1；
   1. $Q = [q1, q2, q3, q4] = W_q * [a1, a2, a3, a4]$
   2. $K = [k1, k2, k3, k4] = W_k * [a1, a2, a3, a4]$
   3. $V = [v1, v2, v3, v4] = W_v * [a1, a2, a3, a4]$
3. 计算相关性：a1作为query方，计算与a2的相关性 $α_{1,2} = q_1 * k_2$ （这里用的是点积），a1总共可以与 a1, a2, a3, a4 求得4个相似性；（q=query，k=key）
   1. $A = [k1, k2, k3, k4]^T * [q1, q2, q3, q4] = K^TQ$，其中 $A_{i,j} = α_{i,j}$
4. 之后可以对4个α做一次标准化为α'，如用softmax；
5. 用4个α'和v计算 $b_1 = \sum α'_{1,i} v_i$
   1. $O = [b1,b2,b3,b4] = [v1, v2, v3, v4]*A’ = VA'$



##### 参数

唯一的参数是 Wq, Wk, Wv 三个矩阵。



##### multi-head 计算相关性

1. q1 = Wq * a1，再乘上另外两个矩阵，得到q11、q12（变成了两个head）；
2. k和v同理；
3. 然后2个head分别计算出b11，b12，拼起来再乘上一个矩阵得到输出



##### Position Encoding（位置信息）

Self-attention没有考虑位置信息，从上面计算相关性时可以看到不同的位置对计算结果没有影响。

可以给输入加上一个位置向量 ai := ai + ei；

ei可以是人为设定的，也可以学习出来，每个位置都有唯一的向量来表示。



##### truncate

一段语言输入很长，向量很多，要计算相关性代价很大，也许不需要考虑所有的输入，而是只对向量附近的几个向量计算相关性。



##### Self-attention vs CNN

$$
CNN \in Self-attention \in FCN
$$

- Self-attention的范围更大，因此需要更多的data，data较少时容易over fitting
- CNN是更有限制的模型，data较少时也较好



其他模型

- RNN：每个输入向量会考虑在它之前的向量，计算相关性（这种结构难以并行计算）Self-attention加上一定限制后就变成了RNN
- GNN：用于graph，也是Self-attention的变形。





# Transformer

是一个sequence-to-sequence（seq2seq）的模型。

输入一个seq，输出一个seq，两者长度不一定相等；输出seq的长度由机器决定（输出树状结构、输出multi-label、）。

如语音识别、机器翻译、语音翻译。



### Encode

输入一个vector-seq，输出一个 vec-seq

encoder结构：

input -> positional encoding -> (Multi-head self-attention -> FC) * N -> output

其中每个self-attention、FC都要进行 add & norm。

add & norm：(输出 + 原始输入) + 做一次 layer normalization



why layer-norm rather than batch-norm?

https://arxiv.org/abs/2003.07845



### Decode



### tips

- copy mechanism：复读、抽取摘要
- guided attention：monotonic attention，location-aware attention
- beam search：避免贪心搜索
- blue score：在optimization无法解决的时候（如loss函数无法微分）：将loss function 作为RL的reward，把decoder作为agent，用RL硬train一发。
- scheduled sampling：在decoder输入里添加一些错误的输入，如“机器”与“机气”
