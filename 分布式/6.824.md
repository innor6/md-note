Distributed Systems

infrastructure - abstraction
  * Storage.
  * Communication.
  * Computation.



# Topic

**Performance**: 

Scalablity

hard: Load im-balance, stragglers, slowest-of-N latency.
    Non-parallelizable code: initialization, interaction.
    Bottlenecks from shared resources

**Fault Tolerance**: 

Availability, Recoverability （冗余、恢复）

replication：非易失性存储（慢），多服务器（同步问题）

**Consistency**:

put() 和 get() 在分布式系统中的语义、问题



# MapReduce

INPUT task -> Map() -> (k, v) --shuffle--> Reduce() -> OUTPUT



# RPC & Threads

why thread？

- I/O concurrency
- Paralleism in multicore
- Convenience

（异步编程——event-driven的单线程：当需要开上百万个线程时，每个线程有自己的栈空间，开销大，异步模型更适合。但编程较困难。）



Thread Challenges

- 共享数据 ："race" 数据竞争 -> mutex, lock
- 线程间协作：Go channels, 条件变量, WaitGroup
- 死锁



# GFS

Why is distributed storage hard?
  <u>high performance</u> -> shard data over many servers
  many servers -> constant faults
  fault tolerance -> replication
  replication -> potential inconsistencies
  better consistency -> <u>low performance</u>



"strong" consistency model

- 行为像单个的服务器
- 使用磁盘存储
- 每次只执行一个客户操作
- 读能够正确反映之前的写，即使服务器崩溃或重启

较差的容错能力。

较好的 consistency 通常需要多个副本服务器之间进行通信，以保持同步，但这会导致性能的降低。



GFS context:

- Global (over a single data center): any client can read any file
     Allows sharing of data among applications
- Automatic "sharding" of each file over many 
- servers/disks
     For parallel performance
     To increase space available
- Automatic recovery from failures
- Just one data center per deployment
- Just Google applications/users
- Aimed at sequential access to huge files; read or append
      I.e. not a low-latency DB for small items



**store**

```
Master state
  in RAM (for speed, must be smallish):
    file name -> array of chunk handles (nv)
    chunk handle -> version # (nv)
                    list of chunkservers (v)
                    primary (v)
                    lease time (v)
  on disk:
    log（文件修改日志）
    checkpoint
```

最新：Master记录每一块的最新版本号，每个chunkserver也记录自己所拥有的chunk的版本号

恢复：恢复到checjpoint，再通过log，复现之后的修改操作



**read steps**

1. C 发送 filename 和 offset 到 master M (if not cached)

2. M 根据 offset 找到对应的 chunk handle

3. M 回复 list of chunkservers（最新版本的）
   
4. C 缓存 handle + chunkserver list

5. C 向最近的 chunkserver 发送请求

   (chunk handle, offset)

6. chunkserver 从磁盘上读 chunk file, 返回



**write append**

primary向全部的secondary发送写命令

如果所有chunk都写成功，则向client返回yes，并更新版本号

如果有一个chunk写失败，则返回no，由客户再次请求写

注：只有一个primary，从而保证只有一个chunkserver在处理写。

“**split brain**”

网络部分的故障，master不能与primary通信，而primary还在与其他client通信。如果服务器指定新的primary，则网络中就同时存在了两个primary服务器。

解决方案：lease（租约）

master和primary都保存了primary的lease。当网络部分故障时，master等待lease失效，失效后再决定新的primary；原来的primary也会因为知道自己lease失效而拒绝client的请求。



# Primary/Backup Replication

fault tolerance

两种主要的 replication 方法：

- State transfer：向副本发送状态，如内存、寄存器（简单，但需要大量的网络传输）
- Replicated state machine：只向副本发送来自外部的 operation / event（只要接收相同的输入，就能计算出相同的输出）

**VMware FT**：

low-level 的 replication，只是内存、寄存器等比特程度上的副本，而不必关心具体的应用逻辑，因此可运行任意的OS和软件。



VMM (virtual machine monitor)：两台物理主机上运行两个VMM，其上分别运行了Primary和Backup虚拟机（物理主机多核，但虚拟机是单核的）。

logging channel：外部事件到来（网络包），运行Primary的VMM把事件也发送给Backup的VMM，备用机上的VMM会丢弃Backup的输出。

两个VMM定期联系，当一段时间未联系时，说明一方崩溃，另一方“goes live”，接收外部事件，并单独提供服务。

**导致不一致的事件**：

- 外部输入：网络数据包、DMA+中断（Signal）
- 时钟中断（1000ms一次？）
- 特殊的指令：随机数、读取当前时间、ip
- 由于是单核，所以不用考虑多核数据竞争

**实现一致**：

1. 对于上述事件，VMM都将通过logging chennel向Backup的VMM发送 log entry（包含：指令号、类型、数据)，用于说明这些特殊指令发生在Primary的CPU执行到哪条指令之后。
2. 这些log entry保存在Backup的VMM的缓冲中，通过在特定的时间执行这些特定的指令，来保证指令的有序进行，包括中断信号。也因此Backup上的指令一定比Primary运行的晚。
3. 对于网路数据，VMM先将虚拟机暂停，当把数据拷贝到内存中，再唤醒虚拟机。

**Output Rule**：

对于网络请求，Primary向Backup发送log entry，Primary的VMM拦截Primary的输出，直到收到Backup的确认ack后，才向客户发回消息。（这种同步降低了性能！）

**Brain Split**：

当网络故障，两个VMM都以为对方崩溃，自己单独“goes live”，再与外部客户通信，将会脑裂。

解决方案：让第三方授权者决定谁来“goes live”，两个VMM同时向这个授权者服务器发送“TEST-AND-SET”（类似信号量、锁），谁先到，谁来当Primary。

（事实上每次有VMM“疑似”崩溃时，即Primary可能换人时，都要进行这个操作）





# RAFT

**背景**
replicate系统中总是需要有办法保证有且只有一个primary server，这是为了防止split brain，但也引入了单点失败

**big insight**
多数表决（majority vote）

- 少数派永远不会成为leader，就没有资格处理请求，也就不会脑裂，
- 2F+1台机器可以容忍F台机器出错
- 当多数派的人员变动时，上一次的多数派与当前多数派，这两个状态中，至少有一台机器是被这两个多数派<u>重叠</u>的。从而新的leader可以通过该机器获得旧leader的状态、信息。

**过程**：

（系统分为两层，应用层和raft层，raft层中有一个log，里面存了多条entry，每个一个entry相当于一个来自client的指令/请求。只要保证每台机器中的entry顺序一致，则执行结果也一致，实现一致性）

1. client请求leader server，
2. leader先不操作，而是把请求entry交给raft层
3. raft层通知其他服务器的raft，添加entry：appendEntry
4. 当收到多数派的回复，则<u>leader可确认entry被成功commit</u>，应用层处理请求，回复客户端
5. 在下一次leader与其他服务器通信时，才顺便把上一次“leader已经收到多数派的回复”这一事实告知其他服务器，这时，<u>其他服务器可以确定那条entry已被commit</u>，随后可以执行。

**leader election**
why leader？有leader更高效

- leader通过与其他server通信，来刷新election计时器（随机时长）。
- 当某个candidate的election计时器过期，说明leader可能失联，则发起election：term++，requestVote
- 每个term，每个服务器只能vote一次yes，以保证最多一个leader（也有可能选不出）。
  - election restriction：服务器只能对term值大于自己的term，或，等于自己的term且log长度大于等于自己，的candidate投yes
- 当一server当选leader后，不立刻通知其他server，而是通过下次发出appendEntry，其他server就知道它当选leader了，（因为只有leader会发送appendEntry）
- 定时器的随机时长，可以保证在第二个candidate参选前，第一个candidate已收集到过半数的选票。

**处理重启机器的log的一致性问题**

每个服务器中保存着这样一个log数组：每个slot（log entry）中记录当前的term和index，以及其他的客户请求信息。

slot可以看作是检查点。

leader通知其他的server进行appendEntry时，

- 发送对应的prevLogIndex和prevLogTerm，
- 每个server从对应的index中比对term是否相同
  - 若不同，则拒绝，leader会倒推到更先前的slot，因此得到更旧的term，收集多个slot的entries，再次发送appendEntry
  - 若相同，则接收，server添加entry到对应slot中（若slot中已有entry，则覆盖，因为leader的slot才是被commit的）。
- leader中存储了每个其他server的状态：nextIndex[serv]
  - 当其他server确认了接收appendEntry，则更新nextIndex
  - 当其他server拒绝appendEntry，则回退nextIndex

**快恢复**

**持久化**

- LOG：相当于服务器的状态
- currentTerm
- votedFor：用于保证每个term只vote一次yes

持久化的时机：为了减少写硬盘次数，可以是在要与其他机器通信时，如PRC，才写入磁盘

**LOG压缩与快照**

如果每次崩溃重启，都要从log的第一条entry开始重新执行，速度会很慢。

观察：应用中的数据状态（如数据库），往往小于log中entry的数量

可以在log中的某个点，记录该点的index+应用状态（快照），持久化后，就可以丢弃先前的log。

这样，LOG的持久化就变成了一个应用快照+部分的LOG。

为了能使leader能够恢复follower的LOG，可以令快照的index不超过【follower中最小的nextIndex】，但这会被最慢的follower拖累，比如这个follower关机一周。

Install Snapshot RPC：另一种方法，leader在发送appendEntry时，发送快照+entry。

**正确性证明：Linearizability**

执行序列是可以线性化的：

存在 某个执行序列，符合实时的非并发的请求顺序

且每个读操作会读取最新写的数据