Distributed Systems

infrastructure - abstraction
  * Storage.
  * Communication.
  * Computation.



# Topic

**Performance**: 

Scalablity

hard: Load im-balance, stragglers, slowest-of-N latency.
    Non-parallelizable code: initialization, interaction.
    Bottlenecks from shared resources

**Fault Tolerance**: 

Availability, Recoverability （冗余、恢复）

replication：非易失性存储（慢），多服务器（同步问题）

**Consistency**:

put() 和 get() 在分布式系统中的语义、问题



# MapReduce

INPUT task -> Map() -> (k, v) --shuffle--> Reduce() -> OUTPUT



# RPC & Threads

why thread？

- I/O concurrency
- Paralleism in multicore
- Convenience

（异步编程——event-driven的单线程：当需要开上百万个线程时，每个线程有自己的栈空间，开销大，异步模型更适合。但编程较困难。）



Thread Challenges

- 共享数据 ："race" 数据竞争 -> mutex, lock
- 线程间协作：Go channels, 条件变量, WaitGroup
- 死锁



# GFS

Why is distributed storage hard?
  <u>high performance</u> -> shard data over many servers
  many servers -> constant faults
  fault tolerance -> replication
  replication -> potential inconsistencies
  better consistency -> <u>low performance</u>



"strong" consistency model

- 行为像单个的服务器
- 使用磁盘存储
- 每次只执行一个客户操作
- 读能够正确反映之前的写，即使服务器崩溃或重启

较差的容错能力。

较好的 consistency 通常需要多个副本服务器之间进行通信，以保持同步，但这会导致性能的降低。



GFS context:

- Global (over a single data center): any client can read any file
     Allows sharing of data among applications
- Automatic "sharding" of each file over many 
- servers/disks
     For parallel performance
     To increase space available
- Automatic recovery from failures
- Just one data center per deployment
- Just Google applications/users
- Aimed at sequential access to huge files; read or append
      I.e. not a low-latency DB for small items



**store**

```
Master state
  in RAM (for speed, must be smallish):
    file name -> array of chunk handles (nv)
    chunk handle -> version # (nv)
                    list of chunkservers (v)
                    primary (v)
                    lease time (v)
  on disk:
    log（文件修改日志）
    checkpoint
```

最新：Master记录每一块的最新版本号，每个chunkserver也记录自己所拥有的chunk的版本号

恢复：恢复到checjpoint，再通过log，复现之后的修改操作



**read steps**

1. C 发送 filename 和 offset 到 master M (if not cached)

2. M 根据 offset 找到对应的 chunk handle

3. M 回复 list of chunkservers（最新版本的）
   
4. C 缓存 handle + chunkserver list

5. C 向最近的 chunkserver 发送请求

   (chunk handle, offset)

6. chunkserver 从磁盘上读 chunk file, 返回



**write append**

primary向全部的secondary发送写命令

如果所有chunk都写成功，则向client返回yes，并更新版本号

如果有一个chunk写失败，则返回no，由客户再次请求写

注：只有一个primary，从而保证只有一个chunkserver在处理写。

“**split brain**”

网络部分的故障，master不能与primary通信，而primary还在与其他client通信。如果服务器指定新的primary，则网络中就同时存在了两个primary服务器。

解决方案：lease（租约）

master和primary都保存了primary的lease。当网络部分故障时，master等待lease失效，失效后再决定新的primary；原来的primary也会因为知道自己lease失效而拒绝client的请求。



# Primary/Backup Replication

fault tolerance

两种主要的 replication 方法：

- State transfer：向副本发送状态，如内存、寄存器（简单，但需要大量的网络传输）
- Replicated state machine：只向副本发送来自外部的 operation / event（只要接收相同的输入，就能计算出相同的输出）

**VMware FT**：

low-level 的 replication，只是内存、寄存器等比特程度上的副本，而不必关心具体的应用逻辑，因此可运行任意的OS和软件。



VMM (virtual machine monitor)：两台物理主机上运行两个VMM，其上分别运行了Primary和Backup虚拟机（物理主机多核，但虚拟机是单核的）。

logging channel：外部事件到来（网络包），运行Primary的VMM把事件也发送给Backup的VMM，备用机上的VMM会丢弃Backup的输出。

两个VMM定期联系，当一段时间未联系时，说明一方崩溃，另一方“goes live”，接收外部事件，并单独提供服务。

**导致不一致的事件**：

- 外部输入：网络数据包、DMA+中断（Signal）
- 时钟中断（1000ms一次？）
- 特殊的指令：随机数、读取当前时间、ip
- 由于是单核，所以不用考虑多核数据竞争

**实现一致**：

1. 对于上述事件，VMM都将通过logging chennel向Backup的VMM发送 log entry（包含：指令号、类型、数据)，用于说明这些特殊指令发生在Primary的CPU执行到哪条指令之后。
2. 这些log entry保存在Backup的VMM的缓冲中，通过在特定的时间执行这些特定的指令，来保证指令的有序进行，包括中断信号。也因此Backup上的指令一定比Primary运行的晚。
3. 对于网路数据，VMM先将虚拟机暂停，当把数据拷贝到内存中，再唤醒虚拟机。

**Output Rule**：

对于网络请求，Primary向Backup发送log entry，Primary的VMM拦截Primary的输出，直到收到Backup的确认ack后，才向客户发回消息。（这种同步降低了性能！）

**Brain Split**：

当网络故障，两个VMM都以为对方崩溃，自己单独“goes live”，再与外部客户通信，将会脑裂。

解决方案：让第三方授权者决定谁来“goes live”，两个VMM同时向这个授权者服务器发送“TEST-AND-SET”（类似信号量、锁），谁先到，谁来当Primary。

（事实上每次有VMM“疑似”崩溃时，即Primary可能换人时，都要进行这个操作）





# RAFT

**背景**
replicate系统中总是需要有办法保证有且只有一个primary server，这是为了防止split brain，但也引入了单点失败

**big insight**
多数表决（majority vote）

- 少数派永远不会成为leader，就没有资格处理请求，也就不会脑裂，
- 2F+1台机器可以容忍F台机器出错
- 当多数派的人员变动时，上一次的多数派与当前多数派，这两个状态中，至少有一台机器是被这两个多数派<u>重叠</u>的。从而新的leader可以通过该机器获得旧leader的状态、信息。

**过程**：

（系统分为两层，应用层和raft层，raft层中有一个log，里面存了多条entry，每个一个entry相当于一个来自client的指令/请求。只要保证每台机器中的entry顺序一致，则执行结果也一致，实现一致性）

1. client请求leader server，
2. leader先不操作，而是把请求entry交给raft层
3. raft层通知其他服务器的raft，添加entry：appendEntry
4. 当收到多数派的回复，则<u>leader可确认entry被成功commit</u>，应用层处理请求，回复客户端
5. 在下一次leader与其他服务器通信时，才顺便把上一次“leader已经收到多数派的回复”这一事实告知其他服务器，这时，<u>其他服务器可以确定那条entry已被commit</u>，随后可以执行。

**leader election**
why leader？有leader更高效

- leader通过与其他server通信，来刷新election计时器（随机时长）。
- 当某个candidate的election计时器过期，说明leader可能失联，则发起election：term++，requestVote
- 每个term，每个服务器只能vote一次yes，以保证最多一个leader（也有可能选不出）。
  - election restriction：服务器只能对term值大于自己的term，或，等于自己的term且log长度大于等于自己，的candidate投yes
- 当一server当选leader后，不立刻通知其他server，而是通过下次发出appendEntry，其他server就知道它当选leader了，（因为只有leader会发送appendEntry）
- 定时器的随机时长，可以保证在第二个candidate参选前，第一个candidate已收集到过半数的选票。

**处理重启机器的log的一致性问题**

每个服务器中保存着这样一个log数组：每个slot（log entry）中记录当前的term和index，以及其他的客户请求信息。

slot可以看作是检查点。

leader通知其他的server进行appendEntry时，

- 发送对应的prevLogIndex和prevLogTerm，
- 每个server从对应的index中比对term是否相同
  - 若不同，则拒绝，leader会倒推到更先前的slot，因此得到更旧的term，收集多个slot的entries，再次发送appendEntry
  - 若相同，则接收，server添加entry到对应slot中（若slot中已有entry，则覆盖，因为leader的slot才是被commit的）。
- leader中存储了每个其他server的状态：nextIndex[serv]
  - 当其他server确认了接收appendEntry，则更新nextIndex
  - 当其他server拒绝appendEntry，则回退nextIndex

**快恢复**

**持久化**

- LOG：相当于服务器的状态
- currentTerm
- votedFor：用于保证每个term只vote一次yes

持久化的时机：为了减少写硬盘次数，可以是在要与其他机器通信时，如PRC，才写入磁盘

**LOG压缩与快照**

如果每次崩溃重启，都要从log的第一条entry开始重新执行，速度会很慢。

观察：应用中的数据状态（如数据库），往往小于log中entry的数量

可以在log中的某个点，记录该点的index+应用状态（快照），持久化后，就可以丢弃先前的log。

这样，LOG的持久化就变成了一个应用快照+部分的LOG。

为了能使leader能够恢复follower的LOG，可以令快照的index不超过【follower中最小的nextIndex】，但这会被最慢的follower拖累，比如这个follower关机一周。

Install Snapshot RPC：另一种方法，leader在发送appendEntry时，发送快照+entry。

**正确性证明：Linearizability**

执行序列是可以线性化的：

存在 某个执行序列，符合实时的非并发的请求顺序

且每个读操作会读取最新写的数据





# zookeeper

**可线性化：（强一致性）**
从用户角度出发，每个请求与应答形成一个时间区间，服务器的操作发生在这个区间里。某种顺序的操作序列是可线性化的，当且仅当不重叠的区间对应的操作有明确的先后顺序，而重叠区间（并发）里的操作可以通过调整顺序，以满足前面所说的操作序列的顺序。（注意：Readx(1)必须是发生在Writex(1)之后的）

允许客户端对任意一个replica发起read-only请求，这使得服务器可能返回一个较旧的值（不满足强一致性），但有时这是可以接受的。这样做主要是为了提升读性能。

**two guarantees：**

1. linearizable writes（写请求严格线性）
2. 2 FIFO client（指单个客户的请求是顺序执行的，即从这个客户的角度看，时间总是单调向前流动的。客户的每次请求都会附带一个zx id，相当于log entry的序号，服务器保证响应这个id（逻辑时钟）之后的应用视图的数据及其zx id，规则一保证了写请求之间的FIFIO，而对于读请求，id保证了每次会比上一次读/写的数据更加新一些）

**配置文件的修改：**
某个服务器中放置了整个分布式系统的配置文件，所有服务器都要去从这里获得更加新的配置。如何保证在修改这个配置文件的过程中，防止其他服务器读到不完整的配置信息？
放置一个ready文件（类似锁），如果该文件被删除，表示正在修改中。
其他服务器读取配置信息时，先检查ready文件是否存在，并附加一个watch消息，以告诉服务器如果ready文件被删除，就通知它。

**atomic：**

**non scalable lock（惊群效应）**

1. if create(file) return（创建锁成功就返回）
2. if exists(file, watch)（锁已经存在则阻塞，并监视）
3. wait
4. goto 1（锁解除后尝试创建锁）

**scalable lock（无惊群效应）**

1. create seq file（创建一个有顺序的锁，如服务端已经存在f24，f25，则创建一个名字为f26的锁）
2. list f*（请求获取所有f开头的锁文件的列表）
3. if no lower # file（如果list中没有比自己的锁更低的序号，获得锁，返回）
4. if exists(next lower # file, watch)（否则阻塞，监控在自己前一位的锁）
5. wait
6. goto 2（更新锁列表）



# CRAQ

**chain replication：**
replica是链式的拓扑。客户发送write请求给head服务器，修改数据后，传递给下一个replica，直到tail，再响应给客户；客户发送read请求给tail来获取数据，当tail能响应这个数据（附带序号）时，说明该客户之前的write请求已经传播到这个最后结点。

优点：链式结构的head的负载要比raft的leader的负载低得多，且链式结构分散了读/写负载。

缺点：链中任意一个服务器过慢，都会导致整个commit变慢；raft中只需大部分机器完成即可，从而隐藏了部分机器的慢速。

类似zookeeper的读，同时保证强一致性。

**recovery：**

链中一个结点失败后，其前驱和后继结点相互链接；如果是head失败，则第二个结点成为head。这种chain模式不能应对网络故障导致的脑裂，实践中可能需要基于其他协议（raft、paxos、zookeeper）的config manager。



# Aurora

amazon-cloud infrastructure-cloud replicated DB
ditched general purpose storage and switch to a very application specific storage system. Amazing 35 times performance increase.

拓扑：一个db服务器，连接6个分布在3个zone存储服务器replica

**优化的措施：**

1. just log entry。db进行持久化时只向replica发送append log entry，速度要比直接传输整个存储数据的页要快的多。（write-ahead logging）
2. quorum。只要有4个replica先完成log记录，就算commit成功，隐藏了低速replica的延迟。

F.T. Goals:

1. write with one dead zone
2. read with on dead zone + one server
3. transient slow
4. fast re-replication
   （quorum配置：N=6, W=4, R=3）

**Quorum system:** 

N个replica，W个用于写，R个用于读，设定R和W的值，以满足W+R=N+1。用户（DB）发送write请求时，只要W个存储服务器响应，就算写入成功；用户read时，只需要R个存储服务器响应，就可以得到最近写的数据（这是由于读写服务器存在交集）。

这种模式类似于raft的majority，同样有利于隐藏dead/slow server带来的延迟。
此外，由于DB server追踪每个存储replica的log版本号，因此对于read请求，可以不使用quorum，而是直接选择一个具有较新版本号的replica，对它单独发起read请求。

对于更大的数据库，可以进行分页分表（sharding），每一部分用6个replica进行存储。

当一个replica崩溃后，要将所有数据复制到一个新replica是很慢的，但是如果一个存储replica实际上是为多个数据库服务，即存储的数据可以分为多个部分，那么当这个replica崩溃后，每个部分对应的各个db都可以分别选择一台新的replica进行备份，从而可以并行的把数据同步到多个新的replica上。



# Frangipani

使用场景：多台workstation共用一个虚拟disk，由于用户主要读写自己的文件，因此在每个workstation上使用frangipani作为cache文件系统。（write-back策略，30s一次）

challenges：

1. cache coherence（一个ws上创建/修改文件后，另一个ws上能立即看到）

   - 规则：不在缓存中的数据没有锁；要从disk读进cache，必须先获得锁；把cache中数据写回disk后，才能释放锁。这保证了读者读数据前，写者已经将数据写回磁盘。（存在一个lock server用于记录每个文件的锁在哪个ws那，每个ws上也有lock table用于记录自己获得的锁）
   - cache-protocol：当有其他用户请求一个被锁的文件时，lock server去请求拥有该锁的ws，如果目标文件处于空闲状态，则写回disk，释放锁

2. atomicity（文件的创建/删除是原子性的，强一致性/可线性化）

     同样用锁实现事务

3. crash recovery（一个ws崩溃，不影响其他ws对文件系统的使用）

   类似Aurora，使用write-ahead log日志，在从cache写回到disk中时，先写回log，然后再写数据块，最后释放锁。这样就可以根据log来恢复损坏的数据。每个log entry的数据都有版本号，从而避免恢复了过时的数据。  